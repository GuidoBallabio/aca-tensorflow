{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for Cifar-10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project we are used:\n",
    "* Python 3.6\n",
    "* Tensorflow 1.8.0\n",
    "* *GPUs parallel calculation manager* nVidia CUDA 9.0\n",
    "* *GPU-accelerated library* nVidia cuDNN 7.1\n",
    "* or CPU optimized tensorflow for intel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first notebook we are going to explain how we have set up our work.  \n",
    "Essentially the process is divided into several parts:\n",
    "* Set up environment with the Cifar-10 Dataset\n",
    "* Define a convolutional neural network\n",
    "* Define a quantization method\n",
    "* Train the convolutional neural network\n",
    "* Provide information about CNN's performance and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daibak/.virtualenvs/aca/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cifar-10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cifar-10 Dataset is taken from the official website www.cs.toronto.edu.\n",
    "\n",
    "Dataset is stored in the data directory: cnn/data. From Cifar-10 dataset we are going to take x_train, t_train, x_test and t_test.\n",
    "The training dataset set is used for training the CNN, the testing dataset is used for evaluate the performance and the accuracy of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn.dense import dataset_preprocessing_by_keras\n",
    "from cnn.utils.dataset import load_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, t_train, x_test, t_test = load_cifar10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 32, 32, 3), (50000, 10), (10000, 32, 32, 3), (10000, 10))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, t_train.shape, x_test.shape, t_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.68747891, -0.65572952, -0.63985482, ..., -0.95734874,\n",
       "        -1.00497283, -0.92559935],\n",
       "       [-0.65572952, -0.63985482, -0.59223073, ..., -0.92559935,\n",
       "        -1.02084753, -0.86210057],\n",
       "       [-0.7192283 , -0.63985482, -0.65572952, ..., -0.68747891,\n",
       "        -0.735103  , -0.70335361],\n",
       "       ...,\n",
       "       [-0.36998499, -0.7509777 , -1.00497283, ..., -0.25886212,\n",
       "        -0.24298742, -0.14773924],\n",
       "       [-0.25886212, -0.65572952, -1.19546919, ...,  0.31262694,\n",
       "        -0.00486698, -0.49698256],\n",
       "       [-0.27473681, -0.35411029, -1.11609571, ...,  0.75711843,\n",
       "         0.64599556,  0.28087755]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = dataset_preprocessing_by_keras(x_train)\n",
    "x_train[0, :, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a custom made wrapper for tensorfllow NN training and use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn.model_class import TfClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This CNN is called *dense_cnn*. Here we will explain how it is composed.\n",
    "\n",
    "The CNN is composed by several layers. In the first part there are 2 **convolutional** layers and 2 **pooling** layers (they are alternated), then there are a *flatten* layer followed by a **relu** layer, a *dropout* layer and finally a **softmax** layer.\n",
    "\n",
    "The network uses a stochastic gradient descent optimizer and a categorical crossentropy loss.  \n",
    "To judge the performance of our model we are used a MSE metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn.dense import NET_NAME, eval_fn, forward_pass, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TfClassifier(NET_NAME, forward_pass, loss_fn, eval_fn,\n",
    "                     tf.train.AdamOptimizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network is trained for 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(\n",
    "#    [x_train, t_train],\n",
    "#    batch_size=64,\n",
    "#    validation_split=0.1,\n",
    "#    epochs=1,\n",
    "#    verbosity=1)\n",
    "#\n",
    "# print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then it's evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/daibak/Documents/Code/Python/aca-tensorflow/cnn/models/dense_cnn/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "evals = model.evaluate([x_test, t_test])\n",
    "\n",
    "print(evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_data_dict_in_perc(input_dict, n_samples, percs):\n",
    "    for k, v in input_dict.items():\n",
    "        input_dict[k] = np.split(v, (n_samples * percs).astype(np.int))\n",
    "\n",
    "    input_LD = [dict(zip(input_dict, t))\n",
    "                for t in zip(*input_dict.values())]  # List of Dicts\n",
    "\n",
    "    return input_LD\n",
    "\n",
    "\n",
    "def _batch_data_dict(input_dict, n_samples, batch_size):\n",
    "    n_batches, drop = np.divmod(n_samples, batch_size)\n",
    "\n",
    "    for k, v in input_dict.items():\n",
    "        input_dict[k] = np.array_split(v, n_batches)\n",
    "\n",
    "    out_LD = [dict(zip(input_dict, t)) for t in zip(*input_dict.values())]\n",
    "\n",
    "    return out_LD\n",
    "\n",
    "\n",
    "def _set_train_mode_to_LD(input_LD, mode):\n",
    "    mode_d = {\"train_mode:0\": mode}\n",
    "    for d in input_LD:\n",
    "        d.update(mode_d)\n",
    "\n",
    "    return input_LD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [x_train, t_train]\n",
    "input_names = [\"features\", \"labels\"]\n",
    "batch_size = 64\n",
    "validation_split = 0.2\n",
    "MAX_BATCH_SIZE = 2000\n",
    "\n",
    "n_samples = inputs[0].shape[0]\n",
    "\n",
    "input_tensors = input_names\n",
    "input_DL = dict(zip(input_tensors, inputs))\n",
    "\n",
    "input_LD = _split_data_dict_in_perc(input_DL, n_samples,\n",
    "                                    np.array([1 - validation_split]))\n",
    "\n",
    "train_dict = input_LD[0]\n",
    "val_dict = input_LD[1]\n",
    "\n",
    "n_train_samples = train_dict[input_tensors[0]].shape[0]\n",
    "\n",
    "train_LD = _batch_data_dict(train_dict, n_train_samples, batch_size)\n",
    "\n",
    "if n_samples - n_train_samples > MAX_BATCH_SIZE:\n",
    "    val_LD = _batch_data_dict(val_dict, n_samples - n_train_samples,\n",
    "                              MAX_BATCH_SIZE)\n",
    "else:\n",
    "    val_LD = [val_dict]\n",
    "\n",
    "train_LD = _set_train_mode_to_LD(train_LD, True)\n",
    "val_LD = _set_train_mode_to_LD(val_LD, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'features'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_LD.extend(input_names)\n",
    "input_LD[-2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aca",
   "language": "python",
   "name": "aca"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
