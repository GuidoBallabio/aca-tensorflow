{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for Cifar-10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project we are used:\n",
    "* Python 3.6\n",
    "* *Keras* from Tensorflow 1.8.0\n",
    "* *GPUs parallel calculation manager* nVidia CUDA 9.0\n",
    "* *GPU-accelerated library* nVidia cuDNN 7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first notebook we are going to explain how we have set up our work.  \n",
    "Essentially the process is divided into several parts:\n",
    "* Set up environment with the Cifar-10 Dataset\n",
    "* Define a convolutional neural network\n",
    "* Define a quantization method\n",
    "* Train the convolutional neural network\n",
    "* Provide information about CNN's performance and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cifar-10 Dataset\n",
    "Cifar-10 Dataset is taken from the official website www.cs.toronto.edu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is stored in the data directory: cnn/data. From Cifar-10 dataset we are going to take x_train, t_train, x_test and t_test.\n",
    "The training dataset set is used for training the CNN, the testing dataset is used for evaluate the performance and the accuracy of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daibak/.virtualenvs/aca/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First sketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This module provides utilities to download, init and load cifar10 dataset.\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import h5py\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "DATA_DIR = 'cnn/data'\n",
    "DATASET_FILE = 'cifar10.h5'\n",
    "\n",
    "\n",
    "def load_dataset_as_tensors():\n",
    "    \"\"\"Load dataset from h5 (not lazily).\"\"\"\n",
    "\n",
    "    h5_filename = Path(DATA_DIR) / DATASET_FILE\n",
    "    with h5py.File(h5_filename.absolute(), 'r') as ds:\n",
    "        return (ds['data/train'][()], ds['label/train'][()],\n",
    "                ds['data/test'][()], ds['label/test'][()])\n",
    "\n",
    "\n",
    "def preprocess_dataset(x_train, t_train, x_test, t_test, NCHW=False):\n",
    "    \"\"\"Preprocess dataset: label -> one hot encodeing, transpose if not in NCHW order.\"\"\"\n",
    "\n",
    "    t_train = keras.utils.to_categorical(t_train).astype(np.uint8)\n",
    "    t_test = keras.utils.to_categorical(t_test).astype(np.uint8)\n",
    "\n",
    "    if not NCHW:\n",
    "        return (x_train.transpose(0, 2, 3, 1), t_train,\n",
    "                x_test.transpose(0, 2, 3, 1), t_test)\n",
    "    else:\n",
    "        return (x_train, t_train, x_test, t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, t_train, x_test, t_test = load_dataset_as_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 3, 32, 32), (50000,), (10000, 3, 32, 32), (10000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, t_train.shape, x_test.shape, t_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uniform data and visualize shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, t_train, x_test, t_test = preprocess_dataset(x_train, t_train, x_test,\n",
    "                                                      t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 32, 32, 3), (50000, 10), (10000, 32, 32, 3), (10000, 10))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, t_train.shape, x_test.shape, t_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This CNN is called *dense_cnn*. Here we will explain how it is composed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN is composed by several layers. In the first part there are 2 **convolutional** layers and 2 **pooling** layers (they are alternated), then there are a *flatten* layer followed by a **relu** layer, a *dropout* layer and finally a **softmax** layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network uses a stochastic gradient descent optimizer and a categorical crossentropy loss.  \n",
    "To judge the performance of our model we are used a MSE metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAIN_SAMPLES = x_train.shape[0]\n",
    "N_TEST_SAMPLES = x_test.shape[0]\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "model = keras.Sequential(\n",
    "    name='dense_cnn',\n",
    "    layers=[\n",
    "        keras.layers.Conv2D(\n",
    "            32,\n",
    "            5,\n",
    "            input_shape=(32, 32, 3),\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu),\n",
    "        keras.layers.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
    "        keras.layers.Conv2D(64, 5, padding='same', activation=tf.nn.relu),\n",
    "        keras.layers.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(1024, activation=tf.nn.relu),\n",
    "        keras.layers.Dropout(0.4),\n",
    "        keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "    ])\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='sgd',\n",
    "    metrics=['accuracy', keras.metrics.mean_squared_error])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network is trained for 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 3/3\n",
      "40000/40000 [==============================] - 139s 3ms/step - loss: 14.4815 - acc: 0.1014 - mean_squared_error: 0.1797 - val_loss: 14.5917 - val_acc: 0.0947 - val_mean_squared_error: 0.1811\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(\n",
    "    x=x_train,\n",
    "    y=t_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=50,\n",
    "    validation_split=0.2,\n",
    "    shuffle=False,\n",
    "    initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': [14.591711572265625],\n",
       " 'val_acc': [0.0947],\n",
       " 'val_mean_squared_error': [0.1810600138902664],\n",
       " 'loss': [14.481510899353028],\n",
       " 'acc': [0.10145],\n",
       " 'mean_squared_error': [0.1797071960926056]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "code_folding": [
     20,
     35,
     40,
     43,
     46,
     64,
     80,
     199,
     202
    ]
   },
   "outputs": [],
   "source": [
    "class SoftmaxClassifier:\n",
    "    def __init__(self,\n",
    "                 name,\n",
    "                 forward_pass_fn,\n",
    "                 loss_fn,\n",
    "                 eval_fn,\n",
    "                 optimizer=tf.train.GradientDescentOptimizer(0.001),\n",
    "                 fake_quantization=False):\n",
    "        self.name = name\n",
    "        self.forward_pass = forward_pass\n",
    "        self.loss_fn = loss_fn\n",
    "        self.eval_fn = eval_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.fake_quantization = fake_quantization\n",
    "        train_ops_graph = self.train_graph()\n",
    "        eval_ops_graph = self.evaluate_graph()\n",
    "        predict_ops_graph = self.predict_graph()\n",
    "        self.tb_path = \"/tmp/log-tb/\" + self.name + \"/\"\n",
    "        self.save_path = \"/tmp/\" + self.name + \".ckpt\"\n",
    "\n",
    "    def infer(self):\n",
    "\n",
    "        self._training_placeholder = tf.placeholder(tf.bool, shape=())\n",
    "\n",
    "        logits = self.forward_pass(\n",
    "            train_mode_placeholder=self._training_placeholder)\n",
    "\n",
    "        predictions = {\n",
    "            \"logits\": logits,\n",
    "            \"classes\": tf.argmax(logits, axis=1, name=\"classes\"),\n",
    "            \"probabilities\": tf.nn.softmax(logits, name=\"softmax\")\n",
    "        }\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def calculate_loss(self, logits):\n",
    "        loss = self.loss_fn(logits=logits)\n",
    "        tf.summary.scalar(\"loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def optimize(self, loss):\n",
    "        return self.optimizer.minimize(loss)\n",
    "\n",
    "    def evaluate_op(self, predictions):\n",
    "        return eval_fn(predictions)\n",
    "\n",
    "    def train_graph(self):\n",
    "        graph = tf.Graph()\n",
    "\n",
    "        with graph.as_default() as g:\n",
    "            predictions = infer()\n",
    "            loss = calculate_loss(predictions[\"logits\"])\n",
    "            train_op = optimize()\n",
    "            evals = evaluate_op(predictions)\n",
    "            summaries = tf.summary.merge_all()\n",
    "\n",
    "        ops = predictions\n",
    "        ops.update(evals)\n",
    "        ops[\"loss\"] = loss\n",
    "        ops[\"summaries\"] = summaries\n",
    "        ops[\"train_op\"] = train_op\n",
    "\n",
    "        return ops, graph\n",
    "\n",
    "    def evaluate_graph(self):\n",
    "        graph = tf.Graph()\n",
    "\n",
    "        with graph.as_default() as g:\n",
    "            predictions = infer()\n",
    "            loss = calculate_loss(predictions[\"logits\"], labels)\n",
    "            evals = evaluate_op(predictions)\n",
    "            summaries = tf.summary.merge_all()\n",
    "\n",
    "        ops = predictions\n",
    "        ops.update(evals)\n",
    "        ops[\"loss\"] = loss\n",
    "        ops[\"summaries\"] = summaries\n",
    "\n",
    "        return ops, graph\n",
    "\n",
    "    def predict_graph(self):\n",
    "        graph = tf.Graph()\n",
    "\n",
    "        with graph.as_default() as g:\n",
    "            predictions = infer()\n",
    "\n",
    "        return predictions, graph\n",
    "\n",
    "    def _split_and_batch(inputs,\n",
    "                         input_names,\n",
    "                         batch_size,\n",
    "                         validation_split):\n",
    "\n",
    "        n_samples = inputs[0].shape\n",
    "        input_tensors = list(map(tf.get_default_graph().get_tensor_by_name,\n",
    "                            input_names))\n",
    "        input_DL = dict(zip(input_tensors, inputs))  # Dict from pairs of list\n",
    "\n",
    "        for k, v in input_DL.items():\n",
    "            input_DL[k] = np.split(v, [round((1 - validation_split) * n_samples)])\n",
    "\n",
    "        input_LD = [dict(zip(input_DL, t))\n",
    "                    for t in zip(*input_DL.values())]  #List of Dicts\n",
    "\n",
    "        train_dict = input_LD[0]\n",
    "        val_dict = input_LD[1]\n",
    "\n",
    "        n_train_samples = train_dict[input_tensors[0]].shape[0]\n",
    "        n_batches, drop = np.divmod(n_train_samples, batch_size)\n",
    "\n",
    "        for k, v in train_dict.items():\n",
    "            train_dict[k] = np.array_split(v, n_batches)\n",
    "\n",
    "        train_LD = [dict(zip(train_dict, t)) for t in zip(*train_dict.values())]\n",
    "\n",
    "        val_dict.update({\"training_placeholder\": False})\n",
    "        mode_t = {\"training_placeholder\": True}\n",
    "        for d in input_LD:\n",
    "            d.update(mode_t)\n",
    "\n",
    "        return train_LD, val_dict\n",
    "    \n",
    "    def fit(self,\n",
    "            inputs,\n",
    "            input_names=[\"features\", \"labels\"],\n",
    "            batch_size=1,\n",
    "            validation_split=0,\n",
    "            epochs=1,\n",
    "            verbosity=0):\n",
    "\n",
    "        ops, graph = self.train_ops_graph\n",
    "        saver = tf.train.Saver()\n",
    "        history = []\n",
    "\n",
    "        with tf.Session(graph=graph) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            train_LD, val_dict = self._split_and_batch(\n",
    "                inputs, input_names, batch_size, validation_split)\n",
    "\n",
    "            val_dict.update({self._training_placeholder: False})\n",
    "\n",
    "            if verbose >= 1:\n",
    "                summary_writer = tf.summary.FileWriter(self.tb_path,\n",
    "                                                       sess.graph)\n",
    "                print(\"Launch: tensorboard --logdir=\" + self.tb_path)\n",
    "                i = 1\n",
    "\n",
    "            for e in range(1, epochs + 1):\n",
    "                sess.run(tf.local_variables_initializer())\n",
    "\n",
    "                for train_dict in train_LD:\n",
    "                    out = sess.run(ops, feed_dict=train_dict)\n",
    "                    if verbosity >= 1:\n",
    "                        summary_writer.add_summary(out[\"summaries\"], i)\n",
    "                        summary_writer.flush()\n",
    "                        i = i + 1\n",
    "                    if verbosity == 2:\n",
    "                        print(out)\n",
    "\n",
    "                out = sess.run(\n",
    "                    {x: ops[x]\n",
    "                     for x in ops if x not in [\"train_op\"]},\n",
    "                    feed_dict=val_dict)\n",
    "\n",
    "                history.append(out)\n",
    "\n",
    "            saver.save(sess, self.save_path)\n",
    "\n",
    "        return dict(\n",
    "            zip(history[0],\n",
    "                zip(*[out.values() for out in history])))  #Dict of lists\n",
    "\n",
    "    def predict(self, inputs, input_names=[\"features\"]):\n",
    "        ops, graph = self.predict_ops_graph\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        input_tensors = map(tf.get_default_graph().get_tensor_by_name,\n",
    "                            input_names)\n",
    "        input_dict = dict(zip(input_tensors, inputs))\n",
    "        input_dict.update({self._training_placeholder: False})\n",
    "\n",
    "        with tf.Session(graph=graph) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver.restore(sess, self.save_path)\n",
    "\n",
    "            out = sess.run(ops, feed_dict=input_dict)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def evaluate(self, inputs, input_name=[\"features\", \"labels\"]):\n",
    "        ops, graph = self.eval_ops_graph\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        input_tensors = map(tf.get_default_graph().get_tensor_by_name,\n",
    "                            input_names)\n",
    "        input_dict = dict(zip(input_tensors, inputs))\n",
    "        input_dict.update({self._training_placeholder: False})\n",
    "\n",
    "        with tf.Session(graph=graph) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver.restore(sess, self.save_path)\n",
    "\n",
    "            out = sess.run(ops, feed_dict=input_dict)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def freeze():\n",
    "        pass\n",
    "\n",
    "    def transform():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def dense_froward_pass(train_mode_placeholder=None):\n",
    "    features = tf.placeholder(\n",
    "        tf.uint8, shape=(None, 32, 32, 3), name=\"features\")\n",
    "\n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs=inputs,\n",
    "        filters=32,\n",
    "        kernel_size=5,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=(2, 2), strides=2)\n",
    "\n",
    "    conv2 = tf.layers.conv2d(\n",
    "        inputs=pool1,\n",
    "        filters=64,\n",
    "        kernel_size=5,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=(2, 2), strides=2)\n",
    "\n",
    "    pool2_flat = tf.layers.flatten(pool2)\n",
    "\n",
    "    dense = tf.layers.dense(\n",
    "        inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "\n",
    "    dropout = tf.layers.dropout(\n",
    "        inputs=dense, rate=0.4, training=train_mode_placeholder)\n",
    "\n",
    "    return tf.layers.dense(inputs=dropout, units=10, name=\"logits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def loss_fn(logits):\n",
    "    labels = tf.placeholder(tf.uint8, [None, 10], name=\"labels\")\n",
    "    return tf.losses.sparse_softmax_cross_entropy(\n",
    "        labels=labels, logits=logits), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def eval_fn(predictions):\n",
    "    labels = tf.get_default_graph().get_tensor_by_name(\"labels:0\")\n",
    "    eval_metrics = {\n",
    "        \"accuracy\":\n",
    "        tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"]),\n",
    "        \"mse\":\n",
    "        tf.metrics.mean_squared_error(\n",
    "            labels=labels, predictions=predictions[\"logits\"])\n",
    "    }\n",
    "    return eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.adam.AdamOptimizer at 0x7f0e7c121c50>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "\n",
    "with g.as_default():\n",
    "    tf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [x_train, t_train]\n",
    "input_names = [\"features\", \"labels\"]\n",
    "batch_size = 64\n",
    "validation_split = 0.2\n",
    "\n",
    "n_samples = inputs[0].shape[0]\n",
    "input_tensors = input_names\n",
    "input_DL = dict(zip(input_tensors, inputs))  # Dict from pairs of list\n",
    "\n",
    "for k, v in input_DL.items():\n",
    "    input_DL[k] = np.split(v, [round((1 - validation_split) * n_samples)])\n",
    "\n",
    "input_LD = [dict(zip(input_DL, t))\n",
    "            for t in zip(*input_DL.values())]  #List of Dicts\n",
    "\n",
    "train_dict = input_LD[0]\n",
    "val_dict = input_LD[1]\n",
    "\n",
    "n_train_samples = train_dict[input_tensors[0]].shape[0]\n",
    "n_batches, drop = np.divmod(n_train_samples, batch_size)\n",
    "\n",
    "for k, v in train_dict.items():\n",
    "    train_dict[k] = np.array_split(v, n_batches)\n",
    "\n",
    "train_LD = [dict(zip(train_dict, t)) for t in zip(*train_dict.values())]\n",
    "\n",
    "val_dict.update({\"training_placeholder\": False})\n",
    "mode_t = {\"training_placeholder\": True}\n",
    "for d in input_LD:\n",
    "    d.update(mode_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 32, 32, 3)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_LD[0][\"features\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.layers.conv2d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aca",
   "language": "python",
   "name": "aca"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
