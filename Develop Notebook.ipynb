{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for Cifar-10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project we are used:\n",
    "* Python 3.6\n",
    "* *Keras* from Tensorflow 1.8.0\n",
    "* *GPUs parallel calculation manager* nVidia CUDA 9.0\n",
    "* *GPU-accelerated library* nVidia cuDNN 7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first notebook we are going to explain how we have set up our work.  \n",
    "Essentially the process is divided into several parts:\n",
    "* Set up environment with the Cifar-10 Dataset\n",
    "* Define a convolutional neural network\n",
    "* Define a quantization method\n",
    "* Train the convolutional neural network\n",
    "* Provide information about CNN's performance and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cifar-10 Dataset\n",
    "Cifar-10 Dataset is taken from the official website www.cs.toronto.edu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is stored in the data directory: cnn/data. From Cifar-10 dataset we are going to take x_train, t_train, x_test and t_test.\n",
    "The training dataset set is used for training the CNN, the testing dataset is used for evaluate the performance and the accuracy of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daibak/.virtualenvs/aca/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\"\"\"This module provides utilities to download, init and load cifar10 dataset.\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "DATA_URL = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "DATA_DIR = 'cnn/data'\n",
    "DATASET_FILE = 'cifar10.h5'\n",
    "\n",
    "\n",
    "def load_dataset_as_tensors():\n",
    "    \"\"\"Load dataset from h5 (not lazily).\"\"\"\n",
    "\n",
    "    h5_filename = Path(DATA_DIR) / DATASET_FILE\n",
    "    with h5py.File(h5_filename.absolute(), 'r') as ds:\n",
    "        return (ds['data/train'][()], ds['label/train'][()],\n",
    "                ds['data/test'][()], ds['label/test'][()])\n",
    "\n",
    "\n",
    "def preprocess_dataset(x_train, t_train, x_test, t_test, NCHW=False):\n",
    "    \"\"\"Preprocess dataset: label -> one hot encodeing, transpose if not in NCHW order.\"\"\"\n",
    "\n",
    "    t_train = keras.utils.to_categorical(t_train).astype(np.uint8)\n",
    "    t_test = keras.utils.to_categorical(t_test).astype(np.uint8)\n",
    "\n",
    "    if not NCHW:\n",
    "        return (x_train.transpose(0, 2, 3, 1), t_train,\n",
    "                x_test.transpose(0, 2, 3, 1), t_test)\n",
    "    else:\n",
    "        return (x_train, t_train,\n",
    "                x_test, t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, t_train, x_test, t_test = load_dataset_as_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 3, 32, 32), (50000,), (10000, 3, 32, 32), (10000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, t_train.shape, x_test.shape, t_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uniform data and visualize shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, t_train, x_test, t_test = preprocess_dataset(x_train, t_train, x_test, t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 32, 32, 3), (50000, 10), (10000, 32, 32, 3), (10000, 10))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, t_train.shape, x_test.shape, t_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This CNN is called *dense_cnn*. Here we will explain how it is composed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN is composed by several layers. In the first part there are 2 **convolutional** layers and 2 **pooling** layers (they are alternated), then there are a *flatten* layer followed by a **relu** layer, a *dropout* layer and finally a **softmax** layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network uses a stochastic gradient descent optimizer and a categorical crossentropy loss.  \n",
    "To judge the performance of our model we are used a MSE metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAIN_SAMPLES = x_train.shape[0]\n",
    "N_TEST_SAMPLES = x_test.shape[0]\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "model = keras.Sequential(\n",
    "    name='dense_cnn',\n",
    "    layers=[\n",
    "        keras.layers.Conv2D(\n",
    "            32,\n",
    "            5,\n",
    "            input_shape=(32, 32, 3),\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu),\n",
    "        keras.layers.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
    "        keras.layers.Conv2D(64, 5, padding='same', activation=tf.nn.relu),\n",
    "        keras.layers.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(1024, activation=tf.nn.relu),\n",
    "        keras.layers.Dropout(0.4),\n",
    "        keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "    ])\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='sgd',\n",
    "    metrics=['accuracy', keras.metrics.mean_squared_error])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network is trained for 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(\n",
    "    x=x_train,\n",
    "    y=t_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=50,\n",
    "    validation_split=0.2,\n",
    "    shuffle=False,\n",
    "    initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python myvenv3",
   "language": "python",
   "name": "myvenv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
