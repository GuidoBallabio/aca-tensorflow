{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for Cifar-10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project we are used:\n",
    "* Python 3.6\n",
    "* Tensorflow 1.8.0\n",
    "* *GPUs parallel calculation manager* nVidia CUDA 9.0\n",
    "* *GPU-accelerated library* nVidia cuDNN 7.1\n",
    "* or CPU optimized tensorflow for intel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first notebook we are going to explain how we have set up our work.  \n",
    "Essentially the process is divided into several parts:\n",
    "* Set up environment with the Cifar-10 Dataset\n",
    "* Define a convolutional neural network\n",
    "* Define a quantization method\n",
    "* Train the convolutional neural network\n",
    "* Provide information about CNN's performance and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cifar-10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cifar-10 Dataset is taken from the official website www.cs.toronto.edu.\n",
    "\n",
    "Dataset is stored in the data directory: cnn/data. From Cifar-10 dataset we are going to take x_train, t_train, x_test and t_test.\n",
    "The training dataset set is used for training the CNN, the testing dataset is used for evaluate the performance and the accuracy of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn.utils.dataset import load_cifar10\n",
    "from cnn.dense import dataset_preprocessing_by_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, t_train, x_test, t_test = load_cifar10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 32, 32, 3), (50000, 10), (10000, 32, 32, 3), (10000, 10))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, t_train.shape, x_test.shape, t_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.68748678, -0.65573727, -0.63986251, ..., -0.95735764,\n",
       "        -1.00498191, -0.92560813],\n",
       "       [-0.65573727, -0.63986251, -0.59223824, ..., -0.92560813,\n",
       "        -1.02085667, -0.8621091 ],\n",
       "       [-0.71923629, -0.63986251, -0.65573727, ..., -0.68748678,\n",
       "        -0.73511105, -0.70336154],\n",
       "       ...,\n",
       "       [-0.36999165, -0.75098581, -1.00498191, ..., -0.25886835,\n",
       "        -0.24299359, -0.14774506],\n",
       "       [-0.25886835, -0.65573727, -1.19547899, ...,  0.31262289,\n",
       "        -0.00487225, -0.4969897 ],\n",
       "       [-0.27474311, -0.35411689, -1.11610521, ...,  0.75711607,\n",
       "         0.64599278,  0.28087337]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = dataset_preprocessing_by_keras(x_train)\n",
    "x_train[0, :, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a custom made wrapper for tensorfllow NN training and use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn.model_class import TfClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This CNN is called *dense_cnn*. Here we will explain how it is composed.\n",
    "\n",
    "The CNN is composed by several layers. In the first part there are 2 **convolutional** layers and 2 **pooling** layers (they are alternated), then there are a *flatten* layer followed by a **relu** layer, a *dropout* layer and finally a **softmax** layer.\n",
    "\n",
    "The network uses a stochastic gradient descent optimizer and a categorical crossentropy loss.  \n",
    "To judge the performance of our model we are used a MSE metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn.dense import NET_NAME, forward_pass, loss_fn, eval_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TfClassifier(NET_NAME, forward_pass, loss_fn, eval_fn,\n",
    "                     tf.train.AdamOptimizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network is trained for 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For training: tensorboard --logdir=/tmp/log-tb/dense_cnn/training\n",
      "For validation: tensorboard --logdir=/tmp/log-tb/dense_cnn/validation\n",
      "{'logits': (array([[-0.5063772 , -4.981158  ,  0.43995675, ..., -0.38799834,\n",
      "        -2.73253   , -5.3762517 ],\n",
      "       [ 3.451639  ,  0.8174111 , -0.80418146, ..., -2.1745582 ,\n",
      "         4.976637  ,  0.8644375 ],\n",
      "       [-1.4941906 ,  5.4328566 , -0.25882745, ..., -3.0065315 ,\n",
      "        -0.9388854 ,  1.785409  ],\n",
      "       ...,\n",
      "       [-1.5114052 , -3.1864355 , -0.04986552, ...,  2.3482933 ,\n",
      "        -3.1718757 , -1.6047599 ],\n",
      "       [-0.6918783 , -1.880009  ,  0.30249122, ...,  2.175767  ,\n",
      "        -2.4612677 , -1.2348982 ],\n",
      "       [-0.37802106, -2.5042136 ,  0.7668707 , ..., -2.3310952 ,\n",
      "         5.109047  , -1.0824846 ]], dtype=float32),), 'classes': (array([3, 8, 1, ..., 7, 4, 8]),), 'probabilities': (array([[8.05834495e-03, 9.18064325e-05, 2.07602903e-02, ...,\n",
      "        9.07103904e-03, 8.69841082e-04, 6.18423583e-05],\n",
      "       [1.73318610e-01, 1.24398321e-02, 2.45790929e-03, ...,\n",
      "        6.24336652e-04, 7.96422243e-01, 1.30388094e-02],\n",
      "       [9.43594903e-04, 9.61975455e-01, 3.24561610e-03, ...,\n",
      "        2.07962134e-04, 1.64418889e-03, 2.50667278e-02],\n",
      "       ...,\n",
      "       [7.32115982e-03, 1.37127133e-03, 3.15731987e-02, ...,\n",
      "        3.47396702e-01, 1.39138289e-03, 6.66862773e-03],\n",
      "       [1.91264097e-02, 5.82954660e-03, 5.16990609e-02, ...,\n",
      "        3.36539447e-01, 3.25984927e-03, 1.11122755e-02],\n",
      "       [4.02911054e-03, 4.80635324e-04, 1.26598729e-02, ...,\n",
      "        5.71478566e-04, 9.73223567e-01, 1.99188665e-03]], dtype=float32),), 'accuracy': ((0.0, 0.6052),), 'mse': ((0.0, 5.791308),), 'loss': (1.1371149,), 'summaries': (b'\\n\\x0b\\n\\x04loss\\x15\\xfb\\x8c\\x91?\\n\\x11\\n\\naccuracy_1\\x15\\x00\\x00\\x00\\x00\\n\\n\\n\\x03mse\\x15\\x00\\x00\\x00\\x00',)}\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [x_train, t_train], batch_size=64, validation_split=0.1, epochs=1, verbosity=1)\n",
    "\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then it's evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = model.evaluate([x_test, t_test])\n",
    "\n",
    "print(evals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aca",
   "language": "python",
   "name": "aca"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
