{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This module provides utilities to download, init and load cifar10 dataset.\"\"\"\n",
    "\n",
    "import pickle\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "DATA_URL = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "DATA_DIR = '.cnn/data'\n",
    "DATASET_FILE = 'cifar10.h5'\n",
    "\n",
    "\n",
    "def download_and_extract():\n",
    "    \"\"\"Download and extract cifar10 dataset.\"\"\"\n",
    "\n",
    "    dest_directory = DATA_DIR\n",
    "    extracted_dir = dest_directory / 'cifar-10-batches-py'\n",
    "\n",
    "    if not dest_directory.exists():\n",
    "        dest_directory.mkdir()\n",
    "\n",
    "    filepath = (dest_directory / DATA_URL.split('/')[-1])\n",
    "\n",
    "    if not filepath.exists() and not extracted_dir.exists():\n",
    "        filepath, _ = urlretrieve(DATA_URL, filepath.absolute())\n",
    "        print('Successfully downloaded')\n",
    "\n",
    "    if not extracted_dir.exists():\n",
    "        tarfile.open(filepath, 'r:gz').extractall(dest_directory)\n",
    "        filepath.unlink()\n",
    "        print('Extracted dataset')\n",
    "\n",
    "\n",
    "def load_arrays_from_pickles(files):\n",
    "    \"\"\"Load shuffled data from pickle files as numpy tensors concateneted.\"\"\"\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for batch in files:\n",
    "        with open(DATA_DIR / 'cifar-10-batches-py' / batch, 'rb') as f_b:\n",
    "            d = pickle.load(f_b, encoding='latin')\n",
    "        data.append(d['data'])\n",
    "        labels.append(np.array(d['labels']))\n",
    "\n",
    "    data = np.concatenate(data)\n",
    "    labels = np.concatenate(labels)\n",
    "    length = len(labels)\n",
    "\n",
    "    data, labels = shuffle(data, labels, random_state=0)\n",
    "\n",
    "    return data.reshape(length, 3, 32, 32), labels\n",
    "\n",
    "\n",
    "def convert_dataset_to_h5():\n",
    "    \"\"\"Convet cifar10 dataset to h5 format.\"\"\"\n",
    "\n",
    "    h5_filename = DATA_DIR / DATASET_FILE\n",
    "\n",
    "    if not h5_filename.exists():\n",
    "        x, t = load_arrays_from_pickles(\n",
    "            (\"data_batch_{}\".format(i) for i in range(1, 6)))\n",
    "        x_test, t_test = load_arrays_from_pickles([\"test_batch\"])\n",
    "\n",
    "        comp_kwargs = {'compression': 'gzip'}\n",
    "\n",
    "        with h5py.File(h5_filename.absolute(), 'w') as f:\n",
    "            f.create_dataset('data/train', data=x, **comp_kwargs)\n",
    "            f.create_dataset(\n",
    "                'label/train', data=t.astype(np.int_), **comp_kwargs)\n",
    "            f.create_dataset('data/test', data=x_test, **comp_kwargs)\n",
    "            f.create_dataset(\n",
    "                'label/test', data=t_test.astype(np.int_), **comp_kwargs)\n",
    "\n",
    "        print('Conversion to HDF5 file successful')\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    \"\"\"Load dataset from h5 (not lazily).\"\"\"\n",
    "\n",
    "    h5_filename = DATA_DIR / DATASET_FILE\n",
    "    with h5py.File(h5_filename.absolute(), 'r') as ds:\n",
    "        return (ds['data/train'][()], ds['label/train'][()],\n",
    "                ds['data/test'][()], ds['label/test'][()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded\n",
      "Extracted dataset\n"
     ]
    }
   ],
   "source": [
    "download_and_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 file successfully  created\n"
     ]
    }
   ],
   "source": [
    "convert_dataset_to_h5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, t_train, x_test, t_test = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 3, 32, 32), (50000,), (10000, 3, 32, 32), (10000,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, t_train.shape, x_test.shape, t_test.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aca",
   "language": "python",
   "name": "aca"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
